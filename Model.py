import json
import time
import random
import subprocess
import argparse
from ollama import chat, ChatResponse
import re
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
import numpy as np

# Configuration
NO_OF_ARTICLES = 3   # Number of articles to process.
NO_OF_CHUNKS = 3     # Number of chunks to select for summarization.
SCRAPER_OUTPUT_FILE = "processed_articles.json"
MODEL_OUTPUT_FILE = "summarized_articles.json"

def load_processed_articles(file_path=SCRAPER_OUTPUT_FILE):
    """Load processed articles from the given JSON file."""
    with open(file_path, "r", encoding="utf-8") as f:
        return json.load(f)

def aggregate_topic_text(topic_dict):
    """
    For a given topic dictionary (with keys "topic_name" and "url_content"),
    concatenate the article texts (ignoring empty ones) into one large string.
    """
    texts = []
    for pair in topic_dict.get("url_content", []):
        # Each pair is expected to be [source_url, article_text]
        article_text = pair[1]
        if article_text.strip():
            texts.append(article_text.strip())
    return "\n".join(texts)

def create_text_chunks(text, chunk_size=1000, chunk_overlap=200):
    """
    Split text into chunks using LangChain's RecursiveCharacterTextSplitter.
    """
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    return splitter.split_text(text)

def create_embeddings_for_chunks(chunks):
    """
    Create embeddings for each text chunk using a local HuggingFace model.
    (These embeddings can be used for further retrieval tasks if needed.)
    """
    hf_embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    embeddings = [hf_embeddings.embed_query(chunk) for chunk in chunks]
    return embeddings

def cosine_similarity(a, b):
    """Compute cosine similarity between two vectors."""
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def extract_title(summary_text: str):
    """
    Extracts the title from the summary text and returns a tuple of (title, cleaned_summary).
    It assumes the output begins with the line:
    
    **Title:** <actual title>
    
    followed by one or more newlines and then the rest of the summary.
    """
    # Updated regex: allow one or more newline characters after the title.
    pattern = r"^\*\*Title:\*\*\s*(.*?)\s*\n+(.*)$"
    match = re.match(pattern, summary_text, re.DOTALL)
    if match:
        title = match.group(1).strip()
        clean_summary = match.group(2).strip()
        return title, clean_summary
    else:
        return "No Title Found", summary_text

def get_chat_response(system_prompt: str, user_prompt: str, model: str = "deepseek-r1:8b") -> str:
    """
    Get a chat response from the specified model using both a system and a user prompt.
    
    Args:
        system_prompt (str): The system message that sets the context and behavior.
        user_prompt (str): The user message with the actual task.
        model (str): The model to use (default: deepseek-r1:8b).
    
    Returns:
        str: The complete output generated by the model.
    """
    response: ChatResponse = chat(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        stream=True,
    )

    full_response = ""
    for chunk in response:
        content = chunk["message"]["content"]
        print(content, end="", flush=True)
        full_response += content
    print("\n")
    # Remove any <think> sections from the response.
    full_response = re.sub(r"<think>.*?</think>", "", full_response, flags=re.DOTALL).strip()
    return full_response

def main(NO_OF_ARTICLES, NO_OF_CHUNKS):
    articles = load_processed_articles(SCRAPER_OUTPUT_FILE)
    articles = articles[:NO_OF_ARTICLES]
    final_results = []

    # System prompt remains unchanged.
    system_prompt = (
        "You are a highly factual and SEO-optimized summarizer. "
        "Your task is to produce concise, authoritative, and fully factual summaries. "
        "Avoid hallucinations and ensure the output includes relevant SEO keywords."
    )

    # Updated user prompt to emphasize the title format.
    user_prompt = (
        "Please produce a blog post summary for the following text. "
        "Your output MUST start with the blog post title exactly in the following format: \n\n"
        "**Title:** <actual title of the blog post>\n\n"
        "Then provide a concise, factual summary below. "
        "Ensure that the summary is SEO optimized with keywords. \n\n"
        "Text: {text}"
    )

    for topic_dict in articles:
        topic_name = topic_dict.get("topic_name", "Unknown Topic")
        print(f"\nProcessing topic: {topic_name}")
        aggregated_text = aggregate_topic_text(topic_dict)
        if not aggregated_text.strip():
            print(f"No text found for topic: {topic_name}")
            continue

        chunks = create_text_chunks(aggregated_text, chunk_size=1000, chunk_overlap=200)
        embeddings = create_embeddings_for_chunks(chunks)

        # Use embeddings: compute centroid and select top N chunks.
        embeddings_arr = np.array(embeddings)
        centroid = embeddings_arr.mean(axis=0)
        similarities = [cosine_similarity(e, centroid) for e in embeddings_arr]
        N = min(NO_OF_CHUNKS, len(chunks))
        top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:N]
        selected_chunks = [chunks[i] for i in top_indices]
        input_text = "\n".join(selected_chunks)

        # Format the user prompt by replacing {text} with our input text.
        formatted_user_prompt = user_prompt.format(text=input_text)

        summary = get_chat_response(system_prompt, formatted_user_prompt, model="deepseek-r1:8b")

        title, clean_summary = extract_title(summary)

        final_results.append({
            "topic_name": topic_name,
            "title": title,
            "summary": clean_summary
        })

    with open(MODEL_OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(final_results, f, indent=4, ensure_ascii=False)

if __name__ == "__main__":
    start_time = time.time()
    main(NO_OF_ARTICLES, NO_OF_CHUNKS)
    end_time = time.time()
    print(f"\nTotal time taken: {end_time - start_time:.2f} seconds.")
